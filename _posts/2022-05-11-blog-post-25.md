---
title: 'Different Recurrent Cells'
date: 2022-05-11
permalink: /posts/2022/05/blog-post-25/
tags:
  - recurrent cell
  - RNN
---

一些不同的递归单元。



**引言**

之前做了一些长记忆张量递归单元的工作，近期发现中文社区的讨论大多集中于简单的RNN LSTM GRU机制上，决定写一些简单的笔记。



**RNN**

最基本的递归单元，更新函数如下：
$$
H_t = g(W_1X_t+W_2H_{t-1}+b_1)\\
O_t = W_3H_t+b_2\\
Y_t = g(O_t)
$$
可以比较直接地看出，连乘会导致RNN单元的梯度消失，即无法获取长时间步长前的序列记忆，也就是所谓的“short memory”。

关于这一点，Bengio等人在 [Learning long-term dependencies with gradient descent is difficult](https://ieeexplore.ieee.org/abstract/document/279181) 这篇文章中做过明晰的论述。他们提到“These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods”，也就是说，在RNN单元下，需要找寻长期记忆能力和学习能力之间的鞍点。



**LSTM**

从现在的角度来看，LSTM作为纯粹的递归单元，结构有一些过度复杂，包括了两个状态H和C的迭代传输，但复杂也有复杂的好处。

LSTM中包括了forget input output三个门，均由
$$
H_{t-1} and X_t
$$
计算得到，用于更新状态和输出，如下：
$$
C_t = g^{forget}C_{t-1}+g^{input}X\\
H^t = g^{output}tanh(C^t)\\
Y^t = sigmoid(WH^t)
$$
显然，LSTM使用了更多的参数，试图提升其长期记忆能力。

在一些简单的实验中，LSTM确实表现出的较好的长期递归学习能力，但是这并不代表其就有了真正的所谓“long memory”。

Zhao等人在2020年做了一个相当有趣的工作，文章名直接叫 [Do RNN and LSTM have long memory?](https://proceedings.mlr.press/v119/zhao20c.html) 他们介绍了长记忆的一个新定义，即“模型权值以多项式速度衰减”。我个人认为这是一项非常有价值的工作，通过统计和数学方法将“long memory”进行了清晰的定义。

基于此，我们可以认为LSTM和RNN的原始形态可以学习和记忆，但是都不存在“long memory”特性。



**RNN2** (two-lane RNN)

RNN2 即 bi-RNN，使用了双向数据进行预测，以实现“基于上下文”预测。

原理与vanilla RNN类似，bi-LSTM同理，此处不过多阐述。



**MRNN** (Memory-augmented RNN)

有好几种模型的缩写都叫MRNN，例如multimodal-RNN，multiplicative-RNN等，此处讨论的是Zhao等人提出的Memory-augmented RNN，源代码[见此](https://github.com/huawei-noah/noah-research/tree/master/mRNN-mLSTM) 。

此处使用了long memory filter实现了长记忆能力，将输入过一遍长记忆过滤器，确保模型权值以多项式速度衰减。其中的记忆参数d可以设置为齐次或者动态值，对应的MLSTM也是类似的结构。

在实验中，这个机制被证明了可以有效提升在长序列记忆任务上的表现。



**HOTRNN** (Higher Order Tensor RNN)

Rose等人提出了[HOTRNN](https://arxiv.org/abs/1711.00073) ,期望通过高阶矩和高阶状态转移函数学习非线性动力学问题，来解决长期预测问题中的时间依赖性，高阶相关性和误差敏感性。

与大多数张量模型类似，因为参数爆炸问题的存在，HOTRNN使用了tensor-train decomposition的方式对高阶张量结构进行分解，以在保持模型性能的同时减少参数数量。



**MIRNN** (Multiplicative integrated RNN)

Bengio的学生Wu等人提出了一种叫做[Multiplicative integration](https://proceedings.neurips.cc/paper/2016/hash/f69e505b08403ad2298b9f262659929a-Abstract.html)的方法用于提升vanilla RNN的表现，用哈德玛积将Wx和Uz两个信息流合并，即
$$
sigmoid((W_x+β_1){\circ}(U_z+β_2)+b)
$$
这个方法可以很方便地应用到诸如RNN，LSTM，GRU等单元上。

若需要MIRNN源代码，可以在Github上以及TensorFlow的issue中找到，该单元已经完成了大部分内容，但最终没有merge到TensorFlow官方库中。



**fTRU** (frational Tensor Recurrent Unit)

我已于先前的文章中介绍[该工作](https://proceedings.mlr.press/v130/qiu21a.html)，同样可以作用于不同的cell上。

该分数阶张量单元同时拥有更强的学习能力与动态记忆的特性，保证了模型的稳定性。