---
title: 'Notes of Beauty of Math'
date: 2021-08-26
permalink: /posts/2021/08/blog-post-16/
tags:
  - math
  - academic
---

近日打算完整地阅读吴军的《数学之美》，做一下笔记。

**Chapter  1 - 文字和语言 vs 数学和信息**

* 信息的冗余是安全的保障
* 语料是翻译的基础，尤其是双语版本
* 最短编码原理 - 高频字符用短码
* 信道窄对应信息的高压缩率 - 例如文言文与白话文
* 语料比语法更重要 - 对翻译系统来说

**Chapter 2 - NLP - 从规则到统计**

* 早期方法 - 语法+词性+构词法
* 上下文相关的文法复杂性极大 - 上下文无关的翻译难度为On^2，相关为On^6
* 统计模型 - 通信系统+隐马尔可夫

**Chapter 3 - 统计语言模型**

* P(s) = P(w1,...,wn) = P(w1)P(w2|w1)...P(wn|w1,w2...wn-1)

​       -> P(w1)P(w2|w1)...P(wn|wn-1) - 二元模型

* P(wn|wn-N+1...wn-1) - N元模型 N-1阶马尔科夫假设

* 语料库中 P(wi|wi-1)≈#(wi-1,wi)/#(wi-1)
* O(|V|^N) - Google使用四元模型 - remains long-dependency problem
* 长尾低频问题：设置阈值，低于则给以权重λ<1，留白作为未知量降低过拟合
* 商业级别翻译系统：大数据量，训练数据和翻译目标匹配，降噪都很重要

**Chapter 4 - 谈谈分词**

* 分词二义性 - 语言歧义性
* P(A1,A2,...) > P(B1,B2,...) - > select A rather B
* 使用动态规划寻找最佳分词
* 此方法主要应用于中文，也可使用于英文手写词分界
* 翻译：大颗粒度更好；搜索：小颗粒度更好
* 分词问题基本上已经完全解决

**Chapter 5 - 隐马尔可夫模型**

* 翻译问题的数学本质 - argmaxP(s1,s2...|o1,o2...) 求s1,s2,...

* 贝叶斯化 - P(o1...|s1...)P(s1...)/P(o1...) 分母可忽略 因为定值

* 马尔可夫 A-0.6->B-1.0->C ...

* 隐马尔可夫：sA->sB->sC

  ​                         oA   oB   oC

* P(s1...|o1...)=PI(t) (St|St-1)P(Ot|St)
  
  P(s1...) = PI(t) P(St|St-1)
  
因此需要计算P(St|St-1) 转移概率 +P(Ot|St) 生成概率
  
* P(Ot|St)≈#(Ot,St)/(St) P(wi|wi-1)≈#(wi-1,wi)/#wi-1 - 有监督学习

* 无监督方法：Banm-Welch 找输出序列的Mθ0模型，计算P(O|Mθ0)，找到所有路径和概率，迭代使P(O|Mθ1)>P(O|Mθ0)，以此寻找局部最优解